{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThis Notebook closely follows the guide presented by Tensorflow here:\\nhttps://www.tensorflow.org/agents/tutorials/1_dqn_tutorial\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This Notebook closely follows the guide presented by Tensorflow here:\n",
    "https://www.tensorflow.org/agents/tutorials/1_dqn_tutorial\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Imports: \"\"\"\n",
    "# Tensorflow Stuff:\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.drivers import py_driver\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import sequential\n",
    "from tf_agents.policies import py_tf_eager_policy\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.policies.epsilon_greedy_policy import EpsilonGreedyPolicy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.metrics import py_metrics\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "\n",
    "import imageio\n",
    "import IPython\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BM stuff:\n",
    "from BMEnvironment import BMEnv\n",
    "from templates.Seesaw_Template import Seesaw\n",
    "from templates.Tower_Template import Tower\n",
    "from templates.Blank_Template import Blank\n",
    "from templates.Roof_Template import Roof\n",
    "from templates.Shield_Template import Shield\n",
    "\n",
    "from templates.PureAvoidance_Template import PureAvoidance\n",
    "from observations.DoubleRaycastObservation import DoubleRayCastObservation\n",
    "from rewards.NewHeightReward import NewHeightReward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_policy_eval_video(policy, env: tf_py_environment.TFPyEnvironment, py_env: BMEnv, filename, num_episodes=1, fps=60, render_obs=False):\n",
    "  \"\"\" Creates a video where actions are mapped out according to the given policy \"\"\"\n",
    "\n",
    "  mode = ''\n",
    "  if render_obs: \n",
    "    mode='render_observation'\n",
    "  else:\n",
    "    mode='render_normal'\n",
    "\n",
    "  filename = filename + \".mp4\"\n",
    "  with imageio.get_writer(filename, fps=fps) as video:\n",
    "    for _ in range(num_episodes):\n",
    "      time_step = env.reset()\n",
    "      video.append_data(py_env.render(mode=mode))                \n",
    "      while not time_step.is_last():\n",
    "        action_step = policy.action(time_step)        \n",
    "        time_step = env.step(action_step)\n",
    "        video.append_data(py_env.render(mode=mode))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Hyper-parameters \"\"\"\n",
    "\n",
    "num_iterations = 15\n",
    "\n",
    "initial_collect_steps = 10\n",
    "\n",
    "collect_steps_per_iteration = 500\n",
    "replay_buffer_max_length = 5000\n",
    "\n",
    "batch_size = 500\n",
    "learning_rate = 5.0e-2\n",
    "gamma = 0.99\n",
    "start_epsilon = 0.15\n",
    "exp_epsilon_decay_base = 0.99\n",
    "grad_clip = 10\n",
    "target_update_period = 1\n",
    "\n",
    "\n",
    "num_eval_episodes = 2\n",
    "eval_interval = 5\n",
    "log_interval = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Create Environments: \"\"\"    \n",
    "\n",
    "# Choose reward function:\n",
    "reward_fn = NewHeightReward()\n",
    "\n",
    "# Choose observation function:\n",
    "observation_fn = DoubleRayCastObservation()\n",
    "\n",
    "# Choose variant ranges:\n",
    "variant_range_train = (0.0, 0.5)\n",
    "variant_range_test = (0.2, 0.2)\n",
    "\n",
    "# Generic Environment:\n",
    "env = BMEnv(    template=Seesaw(), \n",
    "                variantRange=(0.0, 1.0), \n",
    "                reward_fn=reward_fn,\n",
    "                observation_fn=observation_fn  )\n",
    "\n",
    "# Training Environment:\n",
    "train_py_env = BMEnv(   template=Seesaw(), \n",
    "                        variantRange=variant_range_train, \n",
    "                        reward_fn=reward_fn,\n",
    "                        observation_fn=observation_fn  )                        \n",
    "\n",
    "# Evaluation Environment:\n",
    "eval_py_env = BMEnv(    template=Seesaw(), \n",
    "                        variantRange=variant_range_test, \n",
    "                        reward_fn=reward_fn,\n",
    "                        observation_fn=observation_fn  )      \n",
    "\n",
    "# Convert to TF environments:\n",
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)                  \n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Create Agent Q-Net: \"\"\"\n",
    "\n",
    "# Hidden Layers:\n",
    "h_layers = (64, 64)\n",
    "\n",
    "# Action Specification:\n",
    "action_tensor_spec = tensor_spec.from_spec(env.action_spec())\n",
    "num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1\n",
    "\n",
    "\n",
    "def dense_layer(num_units):\n",
    "    \"\"\" Helper function to generate dense layers in Q-net \"\"\"    \n",
    "    return layers.Dense(    num_units, \n",
    "                            activation=tf.keras.activations.relu,\n",
    "                            kernel_initializer=tf.keras.initializers.VarianceScaling(   \n",
    "                                scale=2.0, mode='fan_in', distribution='truncated_normal'\n",
    "                            ))\n",
    "\n",
    "# Create the dense layers:\n",
    "dense_layers = [dense_layer(units) for units in h_layers]   \n",
    "\n",
    "# Create the output layers:\n",
    "output_layer = layers.Dense(    num_actions,\n",
    "                                activation=None,\n",
    "                                kernel_initializer=tf.keras.initializers.RandomUniform(\n",
    "                                    minval=-0.03, maxval=0.03\n",
    "                                ),\n",
    "                                bias_initializer=keras.initializers.Constant(-0.1) )\n",
    "\n",
    "q_net = sequential.Sequential(dense_layers + [output_layer])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Create Agent: \"\"\"\n",
    "# Agent learning optimizer:\n",
    "optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Counter:\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "# Agent:\n",
    "agent = dqn_agent.DqnAgent( optimizer=optimizer,\n",
    "                            time_step_spec=train_env.time_step_spec(),\n",
    "                            action_spec=train_env.action_spec(),\n",
    "                            q_network=q_net,\n",
    "                            td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "                            train_step_counter=train_step_counter,\n",
    "                            gamma=gamma,\n",
    "                            target_update_period=target_update_period,\n",
    "                            target_update_tau=1.0,\n",
    "                            gradient_clipping=grad_clip )\n",
    "\n",
    "agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "    \"\"\"\n",
    "    The standard metric for evaluating a policy in an environment\n",
    "    \"\"\"\n",
    "    total_return = 0.0\n",
    "    iterations = 0\n",
    "    for _ in range(num_episodes):\n",
    "\n",
    "        time_step = environment.reset()\n",
    "        episode_return = 0.0\n",
    "\n",
    "        while not time_step.is_last():\n",
    "            action_step = policy.action(time_step)\n",
    "            time_step = environment.step(action_step.action)\n",
    "            episode_return += time_step.reward\n",
    "            iterations += 1\n",
    "        total_return += episode_return\n",
    "        \n",
    "    avg_return = total_return / iterations\n",
    "    return avg_return.numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Replay Buffer \"\"\"\n",
    "\n",
    "table_name = 'uniform_table'\n",
    "collect_data_spec = tensor_spec.from_spec(agent.collect_data_spec)\n",
    "\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer( data_spec=collect_data_spec,\n",
    "                                                                batch_size=train_env.batch_size,\n",
    "                                                                max_length=replay_buffer_max_length )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Drivers: \"\"\"\n",
    "metric = tf_metrics.AverageReturnMetric()\n",
    "\n",
    "observers = [metric, replay_buffer.add_batch]\n",
    "\n",
    "random_policy = random_tf_policy.RandomTFPolicy(    action_spec=agent.action_spec,\n",
    "                                                    time_step_spec=train_env.time_step_spec()   )\n",
    "\n",
    "dynamic_step_driver.DynamicStepDriver(  env=train_env, \n",
    "                                        policy=random_policy,\n",
    "                                        observers=observers,\n",
    "                                        num_steps=initial_collect_steps).run(time_step=train_env.reset())\n",
    "\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3,\n",
    "    sample_batch_size=batch_size,\n",
    "    num_steps=2).prefetch(3)\n",
    "\n",
    "dataset\n",
    "\n",
    "iterator = iter(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Initialise collect driver: \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Train Agent: \"\"\"\n",
    "\n",
    "starttime = time.time()\n",
    "\n",
    "agent.train = common.function(agent.train)\n",
    "\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "avg_return = compute_avg_return(train_env, agent.policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "\n",
    "time_step = train_env.reset()\n",
    "\n",
    "collect_driver = dynamic_step_driver.DynamicStepDriver( env=train_env,\n",
    "                                                        observers=observers,\n",
    "                                                        policy=agent.collect_policy,                                                        \n",
    "                                                        num_steps=collect_steps_per_iteration   )   \n",
    "iteration = 0\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "\n",
    "    iteration += 1    \n",
    "\n",
    "    epsilon = start_epsilon * pow(exp_epsilon_decay_base, iteration)    \n",
    "    \n",
    "    agent._epsilon_greedy = epsilon\n",
    "\n",
    "    time_step = collect_driver.run()\n",
    "\n",
    "    experience, _ = next(iterator)\n",
    "    train_loss = agent.train(experience).loss\n",
    "\n",
    "    step = agent.train_step_counter.numpy()\n",
    "\n",
    "    if step % log_interval == 0:\n",
    "        print('iteration = {0}: loss = {1}'.format(step, train_loss))\n",
    "\n",
    "    if step % eval_interval == 0:\n",
    "        avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "        print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "        returns.append(avg_return)        \n",
    "\n",
    "timetaken = time.time() - starttime\n",
    "print(\"Time taken: \" + str(timetaken))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = range(0, num_iterations + 1, eval_interval)\n",
    "plt.plot(iterations, returns)\n",
    "plt.ylabel('Average Return')\n",
    "plt.xlabel('Iterations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_policy_eval_video(agent.collect_policy, eval_env, eval_py_env, \"../training_videos/Shield-27-05-2022-5\", 1, render_obs=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1608556bdd26bc524baabcbdd7e1a8d95fe998d67de44c9290c6fa8d90174dda"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('RLIPBM-py_10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
